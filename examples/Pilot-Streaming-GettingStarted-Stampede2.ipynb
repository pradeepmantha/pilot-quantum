{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Pilot-Streaming on Stampede\n",
    "\n",
    "In the first step we need to import all required packages and modules into the Python Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pilot-Streaming utilizes [SAGA-Python](http://saga-python.readthedocs.io/en/latest/tutorial/part3.html) to manage the Spark cluster environment. All attributes of the SAGA Job map 1-to-1 to the Pilot Compute Description. \n",
    "\n",
    "`resource`: URL of the Local Resource Manager. All SAGA adaptors are supported. Examples:\n",
    "\n",
    "* `slurm://localhost`: Submit to local SLURM resource manager, e.g. on master node of Wrangler or Stampede\n",
    "* `slurm+ssh://login1.wrangler.tacc.utexas.edu`: Submit to Wrangler master node SLURM via SSH (e.g. on node running a job)\n",
    "\n",
    "`type:` The `type` attributes specifies the cluster environment. It can be: `Spark`, `Dask` or `Kafka`.\n",
    "\n",
    "\n",
    "Note: This is not required anymore on Stampede 2\n",
    "\n",
    "Depending on the resource there might be other configurations necessary, e.g. to ensure that the correct subnet is used the Spark driver can be configured using various environment variables:   os.environ[\"SPARK_LOCAL_IP\"]='129.114.58.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System LibrariesPilot-Edge-HelloWorld.ipynb\n",
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "\n",
    "## logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    " \n",
    "\n",
    "# Pilot-Streaming\n",
    "import pilot.streaming\n",
    "sys.modules['pilot.streaming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_URL=\"slurm+ssh://login4.stampede2.tacc.utexas.edu\"\n",
    "WORKING_DIRECTORY=os.path.join(os.environ[\"HOME\"], \"work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_compute_description = {\n",
    "    \"resource\":RESOURCE_URL,\n",
    "    \"working_directory\": WORKING_DIRECTORY,\n",
    "    \"number_of_nodes\": 1,\n",
    "    \"cores_per_node\": 48,\n",
    "    \"project\": \"TG-MCB090174\",\n",
    "    \"queue\": \"normal\",\n",
    "    \"config_name\": \"stampede\",\n",
    "    \"walltime\": 59,\n",
    "    \"type\":\"kafka\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kafka_pilot = pilot.streaming.PilotComputeService.create_pilot(pilot_compute_description)\n",
    "kafka_pilot.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_pilot.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_pilot.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributed\n",
    "\n",
    "pilot_compute_description = {\n",
    "    \"resource\":RESOURCE_URL,\n",
    "    \"working_directory\": WORKING_DIRECTORY,\n",
    "    \"number_of_nodes\": 1,\n",
    "    \"cores_per_node\": 48,\n",
    "    \"dask_cores\" : 24,\n",
    "    \"project\": \"TG-MCB090174\",\n",
    "    \"queue\": \"normal\",\n",
    "    \"walltime\": 359,\n",
    "    \"type\":\"dask\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask_pilot = pilot.streaming.PilotComputeService.create_pilot(pilot_compute_description)\n",
    "dask_pilot.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_pilot.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributed\n",
    "dask_client  = distributed.Client(dask_pilot.get_details()['master_url'])\n",
    "dask_client.scheduler_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_client.gather(dask_client.map(lambda a: a*a, range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T18:00:04.950564Z",
     "start_time": "2017-12-28T17:59:22.095228Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### Required Spark configuration that needs to be provided before pyspark is imported and JVM started\n",
    "#os.environ[\"SPARK_LOCAL_IP\"]='129.114.58.101' #must be done before pyspark is loaded\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "pilot_compute_description = {\n",
    "   \"resource\":RESOURCE_URL,\n",
    "    \"working_directory\": WORKING_DIRECTORY,\n",
    "    \"number_of_nodes\": 1,\n",
    "    \"cores_per_node\": 48,\n",
    "    \"project\": \"TG-MCB090174\",\n",
    "    \"queue\": \"normal\",\n",
    "    \"walltime\": 359,\n",
    "    \"type\":\"spark\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Spark Cluster and Wait for Startup Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "spark_pilot = pilot.streaming.PilotComputeService.create_pilot(pilot_compute_description)\n",
    "spark_pilot.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_pilot.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf=pyspark.SparkConf()\n",
    "#conf.set(\"spark.driver.bindAddress\", \"129.114.58.101\")\n",
    "#sc = pyspark.SparkContext(master=\"spark://129.114.58.102:7077\", appName=\"dfas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T17:51:55.430862Z",
     "start_time": "2017-12-28T17:51:55.093479Z"
    }
   },
   "outputs": [],
   "source": [
    "#os.environ[\"SPARK_LOCAL_IP\"]=\"129.114.58.101\"\n",
    "sc = spark_pilot.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd.map(lambda a: a*a).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_pilot.cancel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
